1. Interpretation of Sort Merge Join
   First, the edges from the given database are classified into three groups according to their labels. The groups of edges are stored as three new databases.

   Then, we iterate every edge in the first group and the second group to store the edges from the second group that satisfy firstEdge.to = secondEdge.from into a new database. The process is achieved in the *connectEdges function. We firstly sort the left edges by 'to' values and the second edges by 'from' values in ascending order. We implement our own quick sort function, which can be check at quickSort. Because the join attributes are not unique here, we loop every left edge that has a 'to' value that equals the 'from' value of current right edges.

   We store the count of matching in the max_size of the new result database, and also the edges of matching from the second edges group. We repeat the above step with the previous result and the third edge group to get the new database. The final result will be the max_size of the database.



2. Interpretation of Hash Join

3. Analyse the performance

------------------------------------------------------------------------------------------------------
Benchmark                                                            Time             CPU   Iterations
------------------------------------------------------------------------------------------------------
GraphQueryBenchmark<HashjoinImplementation>/64/32               271170 ns       270566 ns         2582
GraphQueryBenchmark<HashjoinImplementation>/128/32              903044 ns       900719 ns          756
GraphQueryBenchmark<HashjoinImplementation>/256/32             3395734 ns      3356182 ns          212
GraphQueryBenchmark<HashjoinImplementation>/512/32            13215616 ns     13075536 ns           54
GraphQueryBenchmark<HashjoinImplementation>/1024/32           51766439 ns     51632894 ns           13
GraphQueryBenchmark<HashjoinImplementation>/2048/32          204633778 ns    204072275 ns            3
GraphQueryBenchmark<SortMergeJoinImplementation>/64/32          204025 ns       202698 ns         3497
GraphQueryBenchmark<SortMergeJoinImplementation>/128/32         661341 ns       657238 ns          998
GraphQueryBenchmark<SortMergeJoinImplementation>/256/32        2183323 ns      2168346 ns          330
GraphQueryBenchmark<SortMergeJoinImplementation>/512/32        8206694 ns      8175023 ns           86
GraphQueryBenchmark<SortMergeJoinImplementation>/1024/32      33129356 ns     32864105 ns           21
GraphQueryBenchmark<SortMergeJoinImplementation>/2048/32     129867919 ns    129006856 ns            5


Generally speaking,sort merge joins are faster and uses less memory than hash joins. The sort join has a complexity of
O(n^2) + O(m^2) + O(m*n) in the worst case, where n is the size of the left relation and m is the size of the right
relation. The previous O(n^2) + O(m^2) are the complexity of the quick sort process and the O(m*n) is the merge
process. Because the index increases once the matching is found, every tuple in the right relation only goes through once.
In the worst case, every tuples in the right relation goes through the whole left relation. Hence,O(m+n) in the worst case.