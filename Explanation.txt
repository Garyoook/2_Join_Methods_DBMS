1. Interpretation of Sort Merge Join
   The Sort merge join contains the following steps.

   First, the edges from the given database are classified into
   three groups according to their labels. The groups of edges are stored as three new databases.

   Then, we iterate every edges in the first group and second group to store the edges from second group that satisfy
   firstEdge.to = secondEdge.from into a new database. The process is achieved in the
   *connectEdges(const SMDB *first_edges, const SMDB *second_edges) function. We firstly sort the left edges
   by 'to' values and the second edges by 'from' values in ascending order. We implement our own quick sort
   function, which can be check at quickSort(Edge_table **arr, int low, int high, int attribute). Because the join
   attributes are not unique here, we loop every left edges that has a 'to' value that equals the 'from' value of
   current right edges.

   We store the count of matches in the max_size of the new result database, and also the edges of matching from
   the second edges group. We repeat the above step with the previous result and the third edge group to get the new
   database. The final result will be the max_size of the database.


2. Interpretation of Hash Join
    We use the struct Edge_tuple above and a struct HashJoinTable for the Hash join method. HashJoinTable contains
    a list of Edge_tuple called 'edges', a int for its size, and a unsigned long for its maximum allocation size.
    In the allocation stage, the list of Edge is allocated together with the HashJoinDatabase itself.

    At the insertion stage, an Edge_tuple is allocated and assigned with the attributes values input.
    At the edge delete stage, the target edge in 'edges' for deletion is marked as deleted, but it still occupies
    memory space, and this part of memory will be freed when the whole dataset is deleted.

    When we run the query, we allocate 3 empty array of Edge_tuple, for storing tuples with 3 target labels.
    and the size of these 3 arrays are all remembered respectively. Then they are passed into a function 'HashJoin()'
    for the actual Hash join process: first the hash tables of Edge_tuples are initialised with all attributes
    valued -1.
    In the first build phase, the 'to_node' attributes of the first group of edges are hashed using the mod
    hasing method, and the current tuple is stored in the hash table with index of the hash value. The hash value
    calculated is based on slots with a function 'nextSlot' to execute same hash value occurrence.
    Then we probe the build result by edges in the second group by matching the hash values of edges, and build a
    result list of edges with the edges matched.
    Then based on the matching result of previous step, we can build anther hash table of 'from_node' of third group
    of edges and used the 'to_node' of the result edges of previous result set, hence we can have the linked edges
    from the first group to the third group. Finally, we check the remaining edges' 'to_node' with the 'from_node'
    attribute of edges in the first group to finally settle down the triangles. Finally, the counts of the number of
    triangles are returned.

3. Analyse the performance

------------------------------------------------------------------------------------------------------
Benchmark                                                            Time             CPU   Iterations
------------------------------------------------------------------------------------------------------
GraphQueryBenchmark<HashjoinImplementation>/64/32               265184 ns       263796 ns         2816
GraphQueryBenchmark<HashjoinImplementation>/128/32              898149 ns       894980 ns          767
GraphQueryBenchmark<HashjoinImplementation>/256/32             3279742 ns      3269627 ns          215
GraphQueryBenchmark<HashjoinImplementation>/512/32            12359233 ns     12341278 ns           55
GraphQueryBenchmark<HashjoinImplementation>/1024/32           48846264 ns     48778040 ns           14
GraphQueryBenchmark<HashjoinImplementation>/2048/32          194111617 ns    193586851 ns            4
GraphQueryBenchmark<SortMergeJoinImplementation>/64/32          224993 ns       223748 ns         3126
GraphQueryBenchmark<SortMergeJoinImplementation>/128/32         706610 ns       705683 ns          983
GraphQueryBenchmark<SortMergeJoinImplementation>/256/32        2630835 ns      2626714 ns          271
GraphQueryBenchmark<SortMergeJoinImplementation>/512/32       10665703 ns     10606801 ns           69
GraphQueryBenchmark<SortMergeJoinImplementation>/1024/32      42329291 ns     42288111 ns           17
GraphQueryBenchmark<SortMergeJoinImplementation>/2048/32     163205649 ns    163078988 ns            5


Generally speaking,sort merge joins are faster and uses less memory than hash joins. The sort join has a complexity of
O(n^2) + O(m^2) + O(m*n) in the worst case, where n is the size of the left relation and m is the size of the right
relation. The previous O(n^2) + O(m^2) are the complexity of the quick sort process and the O(m+n) is the merge
process. Because the index increases once the matching is found, every tuples in the right relation only go through once.
In the worst case, every tuples in the right relation goes through the whole left relation. Hence,O(m+n) in the worst case.